{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "73d4d3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9b998133",
   "metadata": {},
   "source": [
    "# Loading Data from files onto memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "47484377",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"./\"\n",
    "\n",
    "# Load the data\n",
    "def load_data(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = f.read().split('\\n')\n",
    "    return data\n",
    "\n",
    "train_src = load_data(os.path.join(data_dir, 'train.wp_source'))\n",
    "train_tgt = load_data(os.path.join(data_dir, 'train.wp_target'))\n",
    "val_src = load_data(os.path.join(data_dir, 'valid.wp_source'))\n",
    "val_tgt = load_data(os.path.join(data_dir, 'valid.wp_target'))\n",
    "test_src = load_data(os.path.join(data_dir, 'test.wp_source'))\n",
    "test_tgt = load_data(os.path.join(data_dir, 'test.wp_target'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "580161c4",
   "metadata": {},
   "source": [
    "# Extracting first 10000 for train and 2500 for test and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d3d2dc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_src = train_src[:10000]\n",
    "train_tgt = train_tgt[:10000]\n",
    "val_src = val_src[:2500]\n",
    "val_tgt = val_tgt[:2500]\n",
    "test_src = test_src[:2500]\n",
    "test_tgt = test_tgt[:2500]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0b4dd74a",
   "metadata": {},
   "source": [
    "# Declaring stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ddb7bebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "custom_stopwords = [\"the\", \"a\", \"is\", \"are\", \"was\", \"were\", \"it\", \"in\", \"on\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f9cbbc07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wet', 'marble', 'floor', 'pressed', 'his', 'cheek', 'like', 'thousand', 'hands', 'slapping', 'his', 'face', 'frozen', 'time', 'Smattering', 'piss', 'of', 'rain', 'ignored', 'his', 'indignant', 'mumblings', 'His', 'eyes', 'fluttered', 'Pins', 'and', 'needs', 'ran', 'from', 'finger', 'to', 'shoulder', 'as', 'he', 'pushed', 'back', 'against', 'floor', 'contorting', 'his', 'aching', 'body', 'into', 'cross', 'legged', 'position', 'Last', 'night', 'bad', 'He', 'gathered', 'that', 'His', 'routine', 'dullness', 'of', 'though', 'crept', 'inwards', 'from', 'edges', 'of', 'his', 'mind', 'toward', 'black', 'mist', 'that', 'veiled', 'his', 'most', 'recent', 'memories', 'He', 'struggled', 'to', 'recall', 'whatever', 'he', 'could', \"n't\", 'recall', 'but', 'only', 'for', 'moment', 'before', 'he', 'decided', 'probably', \"n't\", 'worth', 'effort', '<', 'newline', '>', 'He', 'glanced', 'around', 'room', 'for', 'few', 'minutes', 'before', 'concluding', 'that', 'he', 'probably', 'did', \"n't\", 'know', 'where', 'he', 'His', 'investigation', \"n't\", 'entirely', 'fruitless', 'he', 'discovered', 'mostly', 'full', 'bottle', 'of', 'vodka', 'cheap', 'but', 'would', 'definitely', 'get', 'job', 'done', 'Taking', 'few', 'swigs', 'made', 'childishly', 'easy', 'to', 'ignore', 'that', 'gigantic', 'black', 'cloud', 'of', 'fog', 'blotting', 'out', 'whatever', 'hell', 'he', 'did', 'before', 'he', 'woke', 'up', '<', 'newline', '>', 'There', 'mirror', 'room', 'and', 'for', 'want', 'of', 'anything', 'more', 'interesting', 'to', 'study', 'he', 'gazed', 'at', 'himself', 'game', 'he', \"'d\", 'play', 'with', 'himself', 'glancing', 'at', 'mirror', 'and', 'seeing', 'if', 'he', 'could', 'recognize', 'person', 'looking', 'back', 'If', 'he', 'did', \"n't\", 'know', 'better', 'he', \"'d\", 'have', 'guessed', 'he', 'very', 'successful', 'mattress', 'salesman', 'or', 'perhaps', 'bum', 'who', 'had', 'managed', 'to', 'score', 'some', 'luck', 'gambling', '<', 'newline', '>', 'His', 'face', 'portly', 'and', 'unshaven', 'that', 'limbo', 'place', 'where', 'had', 'been', 'too', 'many', 'days', 'without', 'being', 'clean', 'and', 'too', 'few', 'days', 'to', 'become', 'beard', 'His', 'stomach', 'round', 'but', 'firm', 'like', 'basketball', 'stuffed', 'under', 'shirt', 'and', 'then', 'semi', 'deflated', 'hair', 'long', 'and', 'unruly', 'receding', 'far', 'into', 'past', 'But', 'his', 'eyes', 'giveaway', 'Looking', 'closely', 'enough', 'at', 'them', 'he', 'could', 'still', 'see', 'an', 'intensity', \"n't\", 'sharp', 'kind', 'he', 'carried', 'his', 'youth', 'but', 'rather', 'like', 'rusted', 'dagger', 'Still', 'sharp', 'enough', 'to', 'cut', '<', 'newline', '>', '`', '`', 'DiCaprio', 'curse', 'rasped', 'out', 'of', 'him', 'choke', 'After', 'all', 'these', 'years', 'spent', 'working', 'hallmark', 'channel', 'and', 'tv', 'series', 'based', 'mediocre', 'movies', 'he', 'still', 'there', 'Despite', 'his', 'best', 'efforts', 'to', 'bury', 'himself', 'under', 'all', 'of', 'alchol', 'and', 'drugs', 'he', 'still', 'there', 'He', 'thought', 'for', 'sure', 'after', 'bankruptcy', 'he', \"'d\", 'be', 'done', 'but', 'no', 'that', 'god', 'damned', 'rerelease', 'of', 'Titanic', 'royalties', 'started', 'pouring', 'and', 'he', 'could', \"n't\", 'get', 'rid', 'of', 'money', 'Not', 'even', 'live', 'action', 'version', 'of', 'nut', 'job', 'could', 'destroy', 'him', '<', 'newline', '>', 'Cursing', 'he', 'hurled', 'bottle', 'at', 'mirror', 'but', 'his', 'wet', 'hands', 'slipped', 'and', 'instead', 'of', 'shattering', 'crash', 'there', 'only', 'thud', 'as', 'bottle', 'bounced', 'off', 'dry', 'wall', 'and', 'rolled', 'floor', '<', 'newline', '>', 'His', 'rage', 'thwarted', 'by', 'his', 'impotence', 'he', 'slumped', 'against', 'floor', 'and', 'finally', 'noticed', 'why', 'there', 'rain', 'coming', 'into', 'this', 'room', '<', 'newline', '>', '<', 'newline', '>', 'window', 'smashed', 'He', 'looked', 'at', 'bottle', 'confused', 'No', 'he', 'had', \"n't\", 'done', 'that', 'At', 'least', 'not', 'with', 'vodka', 'He', 'looked', 'back', 'at', 'glass', 'etched', 'around', 'window', 'sill', 'and', 'his', 'eyes', 'hung', 'red', 'that', 'stained', 'jagged', 'teeth', '<', 'newline', '>', '<', 'newline', '>', 'headache', 'crept', 'back', 'towards', 'front', 'of', 'his', 'mind', 'while', 'bloody', 'glass', 'pinned', 'his', 'eyes', 'place', 'What', 'fuck', 'happened', 'last', 'night']\n"
     ]
    }
   ],
   "source": [
    "train_src_tokenized = [[token.text for token in nlp(seq) if token.text.lower() not in custom_stopwords and not token.is_punct] for seq in train_src]\n",
    "train_tgt_tokenized = [[token.text for token in nlp(seq) if token.text.lower() not in custom_stopwords and not token.is_punct] for seq in train_tgt]\n",
    "val_src_tokenized = [[token.text for token in nlp(seq) if token.text.lower() not in custom_stopwords and not token.is_punct] for seq in val_src]\n",
    "val_tgt_tokenized = [[token.text for token in nlp(seq) if token.text.lower() not in custom_stopwords and not token.is_punct] for seq in val_tgt]\n",
    "test_src_tokenized = [[token.text for token in nlp(seq) if token.text.lower() not in custom_stopwords and not token.is_punct] for seq in test_src]\n",
    "test_tgt_tokenized = [[token.text for token in nlp(seq) if token.text.lower() not in custom_stopwords and not token.is_punct] for seq in test_tgt]\n",
    "\n",
    "print(test_tgt_tokenized[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "70b14bff",
   "metadata": {},
   "source": [
    "# Dumping to pickle files for colab use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "66a2c2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_dir + 'train_src_tokenized.pickle', 'wb') as f:\n",
    "    pickle.dump(train_src_tokenized, f)\n",
    "    \n",
    "\n",
    "with open(data_dir + 'train_tgt_tokenized.pickle', 'wb') as f:\n",
    "    pickle.dump(train_tgt_tokenized, f)\n",
    "    \n",
    "\n",
    "with open(data_dir + 'val_src_tokenized.pickle', 'wb') as f:\n",
    "    pickle.dump(val_src_tokenized, f)\n",
    "    \n",
    "\n",
    "with open(data_dir + 'val_tgt_tokenized.pickle', 'wb') as f:\n",
    "    pickle.dump(val_tgt_tokenized, f)\n",
    "    \n",
    "\n",
    "with open(data_dir + 'test_src_tokenized.pickle', 'wb') as f:\n",
    "    pickle.dump(test_src_tokenized, f)\n",
    "    \n",
    "\n",
    "with open(data_dir + 'test_tgt_tokenized.pickle', 'wb') as f:\n",
    "    pickle.dump(test_tgt_tokenized, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145b7d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "train_src = \"You\"\n",
    "res = bert_tokenizer.encode(train_src, add_special_tokens=True, max_length=512, truncation=True)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95423941",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "unique_words = set()\n",
    "for seq in train_src:\n",
    "    for word in seq.split(\" \"):\n",
    "        unique_words.add(word)\n",
    "\n",
    "print(sorted(list(unique_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34223894",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "train_src_encoded = [101 2017]\n",
    "res_decode = bert_tokenizer.decode(train_src_encoded, add_special_tokens=True, max_length=512, truncation=True)\n",
    "print(res_decode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa15781a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"./\"\n",
    "with open(data_dir+'train_tgt_tokenized.pickle', 'rb') as f:\n",
    "    train_tgt_tokenized = pickle.load(f)\n",
    "    print(len(train_tgt_tokenized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b09e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(train_tgt_tokenized)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
